# This files contains your custom actions which can be used to run
# custom Python code.
#
# See this guide on how to implement these action:
# https://rasa.com/docs/rasa-pro/concepts/custom-actions

import sys
sys.modules["sqlite3"] = __import__("pysqlite3")
import chromadb
import numpy as np
import google.generativeai as genai
from rasa_sdk import Action, Tracker
from rasa_sdk.executor import CollectingDispatcher
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
import pickle
import os
import re
import spacy

# Load sentence transformer model
model = SentenceTransformer("all-MiniLM-L6-v2")

# Set up Google Gemini API Key
genai.configure(api_key=os.getenv("GENAI_API_KEY"))

# Connect to ChromaDB
VECTOR_DB_PATH = "vector_store"
chroma_client = chromadb.PersistentClient(path=VECTOR_DB_PATH)
collection = chroma_client.get_collection(name="class_materials")

# Load BM25 index
with open("vector_store/bm25_index.pkl", "rb") as f:
    bm25_index, bm25_metadata, bm25_documents = pickle.load(f)

nlp = spacy.load("en_core_web_sm")

def tokenize(query):
    doc = nlp(query.lower())
    tokens = [token.text for token in doc if token.is_alpha and not token.is_stop]
    return tokens

def extract_keywords(query):
    """Extracts only meaningful subject keywords from a query."""
    doc = nlp(query.lower())  # Process query with NLP model
    keywords = []

    #for chunk in doc.noun_chunks:  # Extract noun phrases
    #    if len(chunk.text.split()) > 1:  # Keep multi-word terms (e.g., "DuPont Analysis")
    #        keywords.append(chunk.text)

    for token in doc:  # Extract single important words
        if token.pos_ in {"NOUN", "PROPN"} and not token.is_stop:
            keywords.append(token.text)

    # Remove duplicates while preserving order
    keywords = list(dict.fromkeys(keywords))  
    return keywords

# === ACTION 1: FETCH RAW MATERIAL === #
class ActionFetchClassMaterial(Action):
    def name(self):
        return "action_fetch_class_material"

    def run(self, dispatcher, tracker, domain):
        query = tracker.latest_message.get("text")  # Get user query
        print(f"üßí User query: '{query}'")

        # === DENSE (Vector) SEARCH === #
        query_embedding = model.encode(query, convert_to_numpy=True).tolist()
        vector_results = collection.query(query_embeddings=[query_embedding], n_results=20)

        vector_docs = vector_results["documents"][0]
        vector_metadata = vector_results["metadatas"][0]
        vector_scores = vector_results["distances"][0]  # Lower is better (L2 distance)

        # === SPARSE (BM25) SEARCH === #
        query_tokens = tokenize(query)
        bm25_scores = bm25_index.get_scores(query_tokens)
        top_bm25_indices = np.argsort(bm25_scores)[::-1][:20]  # Top 20 results

        bm25_docs = [bm25_documents[i] for i in top_bm25_indices]
        bm25_metadata_selected = [bm25_metadata[i] for i in top_bm25_indices]
        bm25_scores_selected = [bm25_scores[i] for i in top_bm25_indices]

        # === NORMALIZE SCORES === #
        max_vec_score = max(vector_scores) if vector_scores else 1
        vector_scores = [1 - (score / max_vec_score) for score in vector_scores]  # Convert L2 to similarity

        max_bm25_score = max(bm25_scores_selected) if bm25_scores_selected else 1
        bm25_scores_selected = [score / max_bm25_score for score in bm25_scores_selected]  # Normalize BM25 scores

        # === MERGE & RE-RANK RESULTS === #
        hybrid_results = []
        alpha = 0.7  # Balance factor between vector & keyword search

        for doc, meta, score in zip(vector_docs, vector_metadata, vector_scores):
            hybrid_score = alpha * score + (1 - alpha) * 0  # BM25 score unavailable for vectors
            hybrid_results.append((doc, meta, hybrid_score))

        alpha = 0.3 
        for doc, meta, score in zip(bm25_docs, bm25_metadata_selected, bm25_scores_selected):
            hybrid_score = alpha * 0 + (1 - alpha) * score  # Vector score unavailable for BM25
            hybrid_results.append((doc, meta, hybrid_score))

        # Sort results by hybrid score
        hybrid_results = sorted(hybrid_results, key=lambda x: x[2], reverse=True)

        # === ADAPTIVE THRESHOLDING BASED ON PERCENTILE === #
        scores = [score for _, _, score in hybrid_results]
        max_score = max(scores) if scores else 1
        adaptive_threshold = max(np.mean(scores) + 0.5 * np.std(scores), np.percentile(scores, 80), 0.7 * max_score)
        print(f"\nüìä Adaptive Threshold: {np.mean(scores) + 0.5 * np.std(scores):.3f}, Percentile_80 Threshold: {np.percentile(scores, 80):.3f}, Max Score Threshold: {0.7 * max_score:.3f} \nFinal Threshold => {adaptive_threshold:.3f}")

        # Filter results
        selected_results = [(doc, meta, score) for doc, meta, score in hybrid_results if score >= adaptive_threshold]

        if len(selected_results) == 0:
            dispatcher.utter_message(text="I couldn't find relevant class materials for your query.")
            print("\nüö® No relevant materials found!")
        else: 
            print("\n‚úÖ Selected Pages After Filtering:")
            idx = 1
            for doc, meta, score in selected_results:
                print(f"{idx}. üìÑ PDF: {meta['file']} | Page: {meta['page']} | Score: {score:.4f}")
                idx += 1

            # Format results
            results_text = []
            for text_chunk, meta, score in selected_results:
                file_name = meta["file"]
                page_number = meta["page"]
                results_text.append(f"üìÑ **From {file_name} (Page {page_number})**:\n{text_chunk}")

            # === PREPARE QUERY FOR GEMINI === #
            raw_text = "\n".join(results_text)
            prompt = f"Use the following raw educational content to answer the student query: '{query}'. Make the provided content more readable to the student: \n{raw_text} "

            print("\nüì¢ Sending to Gemini API for Summarization...")
            print(f"üîπ Prompt: {prompt[:300]}...")  # Show only first 300 chars for readability

            try:
                g_model = genai.GenerativeModel("gemini-pro")
                response = g_model.generate_content(prompt)

                if hasattr(response, "text") and response.text:
                    print("\nüéØ Gemini Response Generated Successfully!")
                    dispatcher.utter_message(text=response.text)
                else:
                    print("\n‚ö†Ô∏è Gemini Response is empty.")
                    dispatcher.utter_message(text="Sorry, I couldn't generate a response.")
            except Exception as e:
                dispatcher.utter_message(text="Sorry, I couldn't process that request.")
                print(f"\n‚ùå Error calling Gemini API: {e}")

            # Call the new action for material location
            ActionGetClassMaterialLocation().run(dispatcher, tracker, domain)

        return []

# === ACTION 2: GET PDF NAMES & PAGE LOCATIONS === #
class ActionGetClassMaterialLocation(Action):
    def name(self):
        return "action_get_class_material_location"

    def run(self, dispatcher, tracker, domain):
        query = tracker.latest_message.get("text")  

        #query_tokens = tokenize(query)  # Extract meaningful keywords
        query_tokens = extract_keywords(query)  # Use improved keyword extraction
        print(f"\nüìñ Finding exact material location for query tokens: '{query_tokens}'")
    
        bm25_scores = bm25_index.get_scores(query_tokens)
        top_indices = np.argsort(bm25_scores)[::-1][:10]  # Top 10 matches

        location_results = []
        document_entries = []  # Store documents before sorting

        for i in top_indices:
            file_name = bm25_metadata[i]["file"]
            page_number = bm25_metadata[i]["page"]
            document_text = bm25_documents[i]

            # Tokenize the document text (same as BM25)
            document_tokens = extract_keywords(document_text) #tokenize

            append = True
            for token in query_tokens:
                if token not in document_tokens:
                    append = False
                    break
            if append:
                document_entries.append((file_name, page_number))

        # **Sort by PDF name (A-Z) and then by page number (ascending)**
        document_entries.sort(key=lambda x: (x[0].lower(), x[1]))  

        # Format results
        location_results = [f"üìÑ **{entry[0]} (Page {entry[1]})**" for entry in document_entries]

        if location_results:
            print("\nüéØ Material location for query found!")
            #print("\nüìå FINAL SORTED RESULTS:")
            #for result in location_results:
            #    print(result)
            dispatcher.utter_message(text="You can find more information in:\n" + "\n".join(location_results))
        else:
            print("\n‚ö†Ô∏è  No references to student query related materials found.")
            dispatcher.utter_message(text="I couldn't find specific page references, but check related PDFs.")

        return []
